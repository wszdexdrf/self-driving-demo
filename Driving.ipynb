{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "Driving.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wszdexdrf/self-driving-demo/blob/main/Driving.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWoyijUsZYyM"
      },
      "source": [
        "# Libraries\n",
        "\n",
        "First we import all the libraries needed. We would be creating the model using pytorch. We would also need pandas to handle csv files. We use opencv for image processing. We store the loaded numpy arrays as a hdf5 file since we would have to run the model on the entire clip, as we have RNNs, so we cannot actually hold the entire image dataset in the System memory. "
      ],
      "id": "uWoyijUsZYyM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoQ8_Z0ziMLW"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import os\n",
        "import math\n",
        "import cv2\n",
        "import h5py\n",
        "\n",
        "num_clips = 20\n",
        "base = 'Data\\\\'\n",
        "hfile = h5py.File(base + 'data.hdf5', 'a')"
      ],
      "id": "AoQ8_Z0ziMLW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcbH35EUaw5I"
      },
      "source": [
        "# Loading the Dataset\n",
        "\n",
        "Now we have to load the dataset. As mentioned earlier, the dataset consists of 20 clips, each having the camera feed of a car being driven by a human in a simulator. The feed is from three cameras, centre, left and right. But here we are only using the centre camera images. There is a csv file containing the location of the images and the respective state of the car (throttle, steering angle) of the car in that position. \n",
        "\n",
        "**Note:** the locations of the images are full paths, not relative paths, therefore to run this part of the code, one needs to adjust them, or place the \"data\" folder according to the paths in the csv files and just \"Find and replace\" the username from all the paths at once. This might not actually be needed to train the model since the hdf5 file is already provided."
      ],
      "id": "LcbH35EUaw5I"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "falling-neutral"
      },
      "source": [
        "# The function load_img_steering is used to load all the image_paths and convert it into a numpy array.\n",
        "# It also loads the steering values, which is the target value for us, and returns it as another numpy \n",
        "# array. After this, we have two numpy arrays for each clip, one containing the paths of all images in \n",
        "# the clip, and another containing the target values.\n",
        "\n",
        "def load_img_steering(datadir, df):\n",
        "  image_path = []\n",
        "  steering = []\n",
        "  for i in range(len(df)):\n",
        "    indexed_data = df.iloc[i]\n",
        "    center, left, right = indexed_data[0], indexed_data[1], indexed_data[2]\n",
        "    image_path.append(os.path.join(datadir, center.strip()))\n",
        "    steering.append(float(indexed_data[3]))\n",
        "  image_paths = np.asarray(image_path)\n",
        "  steerings = np.asarray(steering)\n",
        "  return image_paths, steerings\n",
        "\n",
        "for i in range(num_clips):\n",
        "\n",
        "  # The location of each clip is the path to the base variable define above + the clip number. \n",
        "  # (Note that the base variable includes a extra \"\\\\\") .\n",
        "  datadir = base + str(i+1)\n",
        "\n",
        "  # The columns in the csv files are included in the list named \"columns\".\n",
        "  columns = ['center', 'left', 'right', 'steering', 'throttle', 'reverse', 'speed']\n",
        "  \n",
        "  data = pd.read_csv(os.path.join(datadir, 'driving_log.csv'), names = columns)\n",
        "  image_paths, steering = load_img_steering(datadir, data)\n",
        "\n",
        "  # The size of the input dataset would be the length of this 1-D array of image_paths concatenated with the \n",
        "  # dimensions of each image, which would be (160,320, 3) in our case. Since we need CHW format, therefore \n",
        "  # the final dataset will be of the shape (len, 3, 160, 320).\n",
        "  size = np.shape(image_paths) + (3, 160, 320)\n",
        "  imgs = np.zeros(size)\n",
        "  \n",
        "  \n",
        "  # We read the images for each path and store this. Then we store the entire numpy array (len, 3, 160, 320) \n",
        "  # as a dataset in our hdf5 file, with the name 1x, 2x, etc. The target values are stored as 1y, 2y, etc. \n",
        "  # respectively. This hdf5 file resides on permanent storage, since the entire dataset cannot be stored in memory.\n",
        "  for j,x in enumerate(image_paths):\n",
        "    imgs[j] = np.transpose(cv2.imread(x)[60::160,:,:], (2, 0, 1))/255\n",
        "  xset = hfile.create_dataset(str(i + 1) + 'x', np.shape(imgs), h5py.h5t.STD_U8BE, data = imgs)\n",
        "  yset = hfile.create_dataset(str(i + 1) + 'y', np.shape(steering), data = steering)"
      ],
      "id": "falling-neutral",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-7Th5TOe62y"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "The model consists of 5 consecutive CNN layers and then 2 Dense layers. The output of this sequential model is then given to simple RNN cell. The result is finally passed through a Linear layer which converts the output to a single scalar value. The exact specifications are given below in the code."
      ],
      "id": "T-7Th5TOe62y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "departmental-geneva"
      },
      "source": [
        "import torch\n",
        "import time\n",
        "\n",
        "class Driver(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Driver, self).__init__()\n",
        "    self.net = torch.nn.Sequential(\n",
        "      torch.nn.Conv2d(3, 24, kernel_size = 5, stride = 2),\n",
        "      torch.nn.Conv2d(24, 36, kernel_size = 5, stride = 2),\n",
        "      torch.nn.Conv2d(36, 48, kernel_size = 5, stride = 2),\n",
        "      torch.nn.Conv2d(48, 64, kernel_size = 3),\n",
        "      torch.nn.Conv2d(64, 64, kernel_size = 3),\n",
        "      torch.nn.Flatten(),\n",
        "      torch.nn.Linear(27456, 100),\n",
        "      torch.nn.Linear(100, 50)\n",
        "      )\n",
        "    self.cell = torch.nn.RNNCell(50, 5, nonlinearity = 'relu')\n",
        "    self.out_layer = torch.nn.Linear(5, 1)\n",
        "  \n",
        "  def forward(self, img, hx):\n",
        "    net_out = self.net(img)\n",
        "    hidden = self.cell(net_out, hx)\n",
        "    return self.out_layer(hidden), hidden\n",
        "\n",
        "model = Driver()\n",
        "\n",
        "# We are using Mean Square Error as our loss function, since we have to \"regress\" \n",
        "# to the optimum steering value for the given situation. We are using Adam optimizer.\n",
        "loss_fn = torch.nn.functional.mse_loss\n",
        "opt = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
        "\n",
        "hfile = h5py.File(base + 'data.hdf5', 'r+')\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for i in range(num_clips):\n",
        "\n",
        "    print(\"Clip \", i+1, \"Epoch \", epoch+1)\n",
        "    t = time.time()\n",
        "    \n",
        "    # For each clip in each epoch, we load the images and target values into the \n",
        "    # system memory\n",
        "    images = torch.Tensor(hfile['/' + str(i + 1) + 'x']).float()\n",
        "    steerings = torch.Tensor(hfile['/' + str(i + 1) + 'y']).float()\n",
        "\n",
        "    # Printing the time spent in loading the clip into memory\n",
        "    print(\"Reading \", time.time() - t)\n",
        "\n",
        "    num_images = int(images.shape[0])\n",
        "    \n",
        "    # Initializind the hidden state\n",
        "    hx = torch.zeros(1, 5).float()\n",
        "\n",
        "    total_loss = 0\n",
        "    \n",
        "    # Combine the Tensors into a TensorDataset object\n",
        "    train_ds = torch.utils.data.TensorDataset(images, steerings)\n",
        "\n",
        "    t = time.time()\n",
        "    for index, (x, y) in enumerate(train_ds):\n",
        "\n",
        "      # Model expects 4D input\n",
        "      x = torch.unsqueeze(x, 0)\n",
        "\n",
        "      # Predictions of the model.\n",
        "      pred, hx = model(x, hx)\n",
        "      \n",
        "      # Computing the loss\n",
        "      loss = loss_fn(pred, y)\n",
        "\n",
        "      # Calculating total loss for displaying average loss for the clip. Serves \n",
        "      # no purpose other than evaluation.\n",
        "      total_loss = total_loss + float(loss)\n",
        "\n",
        "      # Displaying the current average loss\n",
        "      print('\\r', end = '')\n",
        "      print(\"Loss \", loss.item(), end= '')\n",
        "\n",
        "    # Printing the time spent in forward pass\n",
        "    print(\"\\nForward \", time.time() - t)\n",
        "    \n",
        "    t = time.time()\n",
        "\n",
        "    # Bacpropagation of gradients and optimizing the model\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "    opt.zero_grad()\n",
        "\n",
        "    # Printing the time spent in Backpropagation\n",
        "    print(\"Backpropagation \", time.time() - t, end='\\n-------------------------------\\n')"
      ],
      "id": "departmental-geneva",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYcoslR-uYA_"
      },
      "source": [
        "# Saving the Model\n",
        "\n",
        "We are saving the trained model as in the ONNX format."
      ],
      "id": "YYcoslR-uYA_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsHwZIvSNWcT"
      },
      "source": [
        "# Dummy inputs \n",
        "x, hx = torch.randn(1, 3, 160, 320), torch.randn(1, 5)\n",
        "\n",
        "# Names of inputs\n",
        "input_names, output_names = [\"images\", \"hidden_state\"],[\"steerings\"]\n",
        "\n",
        "# Testing if dummy inputs are correct \n",
        "pred, hidden = model(x, hx)\n",
        "\n",
        "# Exporting the model. The dummy inputs are used to save the format of inputs \n",
        "# to the model.\n",
        "torch.onnx.export(model, (x, hx), \"Driver.onnx\")"
      ],
      "id": "GsHwZIvSNWcT",
      "execution_count": null,
      "outputs": []
    }
  ]
}